{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03eec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pythainlp\n",
    "# !pip install wordfreq\n",
    "# !pip install mecab\n",
    "# !apt-get install mecab mecab-ipadic-utf8 libmecab-dev swig\n",
    "# !pip install mecab-python3\n",
    "# !pip install mecab-ko\n",
    "# !pip install ipadic\n",
    "# !pip install git+https://github.com/TalkBank/TBDBpy.git\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import codecs\n",
    "import random\n",
    "import shutil\n",
    "import ssl\n",
    "import time\n",
    "import zipfile\n",
    "from string import punctuation\n",
    "from io import StringIO\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "import urllib3\n",
    "from lxml import etree\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize as nltk_tokenize\n",
    "from pythainlp import word_tokenize as thai_tokenize\n",
    "from requests.exceptions import Timeout, ConnectionError\n",
    "from urllib3.exceptions import ReadTimeoutError\n",
    "from wordfreq import tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import MeCab\n",
    "import mecab_ko_dic\n",
    "\n",
    "import tbdb\n",
    "lang_dic = { \n",
    "     'zh': ['Mandarin_cmn', 'cmn', 'Mandarin', 'Mandarin Chinese', 'Hans', 'Chinese'],\n",
    "}\n",
    "\n",
    "report = codecs.open('../report_conversationSLR123.csv', 'a', 'utf-8')\n",
    "report.write('folder_name;tokens;txt_files\\n')\n",
    "\n",
    "resource = ''\n",
    "copyright_short = ''\n",
    "copyright_long = ''\n",
    "duplicate_text_set = set()\n",
    "\n",
    "total_tokens = 0\n",
    "first_counter = 0\n",
    "current_counter = 0\n",
    "\n",
    "\n",
    "\n",
    "# get content from link\n",
    "def get_content(link):\n",
    "  content = ''\n",
    "  try:\n",
    "    with open(link, 'r', encoding = 'utf-8') as file:\n",
    "        tsv_lines = file.readlines()\n",
    "        for line in tsv_lines:\n",
    "            line = line.replace('\\t', ' ')\n",
    "        content = '\\n'.join(tsv_lines)\n",
    "  except FileNotFoundError:\n",
    "    print(\"The file does not exist or the path is incorrect.\")\n",
    "  except Exception as e:\n",
    "    print(\"An error occurred:\", str(e))\n",
    "  return content\n",
    "\n",
    "\n",
    "# get date when resouce was created\n",
    "def parse_date(content):\n",
    "  return '2022'\n",
    "\n",
    "\n",
    "# read text line by line and remove everything before last space (speaker id)\n",
    "def remove_speakers_names(text):\n",
    "  lines = text.split('\\n')\n",
    "  result = []\n",
    "\n",
    "  for line in lines:\n",
    "      result.append(line.split('\t')[-1].strip())\n",
    "\n",
    "  result_text = '\\n'.join(result)\n",
    "  return result_text\n",
    "\n",
    "\n",
    "# filtering empty lines and spaces\n",
    "def filter_text(text):\n",
    "  lines = text.split('\\n')\n",
    "  non_empty_lines = [line for line in lines if line.strip()]\n",
    "  text = '\\n'.join(non_empty_lines)\n",
    "\n",
    "  stripped_lines = [line.strip() for line in text.split('\\n')]\n",
    "  text = '\\n'.join(stripped_lines)\n",
    "\n",
    "  return text\n",
    "\n",
    "def extract50K(text, lang):\n",
    "    tokens_extracted = 0\n",
    "    text = StringIO(text)\n",
    "    text50K = ''\n",
    "    remaining_text = ''\n",
    "    while(True):\n",
    "        nl = text.readline()\n",
    "        if nl != '':\n",
    "            tokens_extracted += count_tokens(nl, lang)[0]\n",
    "            if(tokens_extracted < 50000):\n",
    "                text50K += nl\n",
    "            else:\n",
    "                remaining_text += nl\n",
    "        else:\n",
    "            break;\n",
    "    return text50K, remaining_text\n",
    "\n",
    "\n",
    "def define_line_start(arr_text):\n",
    "    random_range = [0]\n",
    "    for i in range(int(len(arr_text)/2)):\n",
    "        if arr_text[i][-1] in ['?','.']:\n",
    "            char_start = i\n",
    "            random_range.append(char_start)\n",
    "    char_start = random.choice(random_range)\n",
    "    return char_start\n",
    "\n",
    "# Random starting point\n",
    "def starting_point(arr_text, tokens, lang):\n",
    "    if tokens > 100001:\n",
    "        line_start = define_line_start(arr_text)\n",
    "        return line_start\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def write_50k_to_file(fname,lang_dic,lang,year,resource,link,text,isWhole):\n",
    "    f1 = codecs.open(fname, 'w', 'utf-8')\n",
    "    meta = '''# language_name_wals:\t''' + lang_dic[lang][2] + '''\n",
    "# language_name_glotto:\t''' + lang_dic[lang][3] + '''\n",
    "# iso639_3:\t''' + lang_dic[lang][1] + '''\n",
    "# year_composed:\tNA\n",
    "# year_published:\t''' + year + '''\n",
    "# mode:\twritten\n",
    "# genre_broad:\tconversation\n",
    "# genre_narrow:\tNA\n",
    "# writing_system:\t''' + lang_dic[lang][4] + '''\n",
    "# special_characters:\tNA\n",
    "# short_description:\t''' + resource + '''\n",
    "# source:\t''' + copyright_short + '''\n",
    "# copyright_short:\t''' + copyright_short + '''\n",
    "# copyright_long:\t''' + copyright_long + '''\n",
    "# sample_type:\t''' + isWhole + '''\n",
    "# comments:\tNA\n",
    "'''\n",
    "    meta = re.sub('\\t{2,}', '\\t', meta)\n",
    "    meta = re.sub(' {2,}', ' ', meta)\n",
    "    f1.write(meta)\n",
    "\n",
    "    text = re.sub('\\t+', ' ', text)\n",
    "    text = re.sub(' {2,}', ' ', text)\n",
    "\n",
    "    f1.write(text[:-1])\n",
    "    f1.close()\n",
    "\n",
    "# Root path\n",
    "def get_root(lang):\n",
    "    pathToZip = os.path.join('..','Corpus',lang_dic[lang][0], 'conversation' )\n",
    "\n",
    "    if not os.path.exists(pathToZip):\n",
    "        os.makedirs(pathToZip)\n",
    "    return pathToZip\n",
    "\n",
    "# Generate file name\n",
    "def generate_fname(lang, current_counter):\n",
    "    path = get_root(lang)\n",
    "    max_counter = 0\n",
    "    search_fcounter = re.compile(lang_dic[lang][1] + '_con_' + '([0-9]+)(\\\\.txt)?')\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for fname in files:\n",
    "            fcounter = re.search(search_fcounter, fname)\n",
    "\n",
    "            if fcounter is not None:\n",
    "                if int(fcounter.group(1)) > max_counter:\n",
    "                    current_counter = fcounter.group(1)\n",
    "                    max_counter = int(current_counter)\n",
    "            else:\n",
    "                current_counter = 0\n",
    "    if max_counter > 0:\n",
    "        current_counter = max_counter\n",
    "    fname = lang_dic[lang][1] + '_con_' + str(int(current_counter) + 1) + '.txt'\n",
    "    return os.path.join(get_root(lang) ,fname), current_counter\n",
    "\n",
    "# Sample\n",
    "def sample(link, text, year, tokens, lang, current_counter, total_tokens,tokens_arr):\n",
    "    fname = ''\n",
    "    if int(tokens) <= 50000 and int(tokens) > 0:\n",
    "        fname, current_counter = generate_fname(lang, current_counter)\n",
    "        write_50k_to_file(fname,lang_dic,lang,year,resource,link,text,'whole')\n",
    "        total_tokens += int(tokens)\n",
    "\n",
    "    if(tokens>50000):  \n",
    "        whole_file_covered = 0\n",
    "        while not whole_file_covered:\n",
    "            starting_place = starting_point(text, tokens, lang)\n",
    "            if (starting_place!=0):\n",
    "                starting_place+=1\n",
    "                \n",
    "            left, right = text[:starting_place], text[starting_place:]\n",
    "            text_with_only_50k, remaining_text = extract50K(right, lang)\n",
    "            remaining_text = left + remaining_text          \n",
    "\n",
    "            fname, current_counter = generate_fname(lang, current_counter)\n",
    "            write_50k_to_file(fname,lang_dic,lang,year,resource,link,text_with_only_50k,'part')\n",
    "            tokens = count_tokens(text_with_only_50k, lang)[0]\n",
    "            total_tokens += tokens\n",
    "            text = remaining_text\n",
    "            tokens_arr = count_tokens(text, lang)[1]\n",
    "            token_cnt, t = count_tokens(text, lang)\n",
    "\n",
    "            if(token_cnt < 100):\n",
    "                whole_file_covered = 1\n",
    "                   \n",
    "    return total_tokens, current_counter, fname, tokens\n",
    "\n",
    "# Tokenization\n",
    "def count_tokens(text, lang):\n",
    "    if(lang =='zh_CN'):\n",
    "        lang = 'zh'\n",
    "    if(lang == 'en_GB'):\n",
    "        lang = 'en'\n",
    "    if lang in ['en', 'fi', 'fr', 'de', 'el', 'he', 'hi', 'id', 'ja', 'ko', 'zh','fa', 'ru', 'es', 'tr']:\n",
    "        tokens = tokenize(text, lang)\n",
    "    elif lang == 'th':\n",
    "        tokens = thai_tokenize(text)\n",
    "    else:\n",
    "        tokens = nltk_tokenize(text)\n",
    "\n",
    "    tokens_tmp = [word.lower() for word in tokens\n",
    "              if not (word in punctuation and not word.isdigit() and not word.isspace())]\n",
    "\n",
    "    return len(tokens_tmp),tokens\n",
    "\n",
    "# Retrieve text from document and filter it\n",
    "def get_text(lang, checkForDuplicates, content):\n",
    "    if(not content):\n",
    "      return False\n",
    "\n",
    "    text_for_checking_duplicates = ''\n",
    "    num_line = 0\n",
    "\n",
    "    year = parse_date(content)\n",
    "    content = remove_speakers_names(content)\n",
    "    text = filter_text(content)\n",
    "\n",
    "    text_for_checking_duplicates = tuple(text.splitlines()[:20])\n",
    "\n",
    "    tokens, tokens_arr = count_tokens(text, lang)\n",
    "\n",
    "    if(checkForDuplicates):\n",
    "        if(text_for_checking_duplicates in duplicate_text_set):\n",
    "            return False\n",
    "        else:\n",
    "            duplicate_text_set.add(text_for_checking_duplicates)\n",
    "        \n",
    "    return text, year, tokens, tokens_arr\n",
    "\n",
    "def total_tokens_not_reached(total_tokens):\n",
    "    if ((total_tokens > 5000000)):\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "def parse_data(lang, total_tokens,current_counter):\n",
    "    tokens_per_resource_per_language = 0\n",
    "    folderPath = './TXT'\n",
    "    pathToFiles = os.listdir(folderPath)\n",
    "    \n",
    "    for pathToFile in pathToFiles:\n",
    "        link = os.path.join(folderPath, pathToFile)\n",
    "\n",
    "        text = get_content(link)\n",
    "\n",
    "        if total_tokens_not_reached(total_tokens):\n",
    "            if get_text(lang, True, text) is not False:\n",
    "                text, year, tokens, tokens_arr = get_text(lang, False, text)\n",
    "\n",
    "                total_tokens, current_counter, fname, current_tokens = sample(link, text, year, tokens, lang,\n",
    "                                                              current_counter, total_tokens, tokens_arr)\n",
    "                tokens_per_resource_per_language += current_tokens\n",
    "                print(\"Saved file\", fname)\n",
    "\n",
    "            else:\n",
    "                print('Skipped file due to parsing error or duplicate file')\n",
    "        else:\n",
    "            return total_tokens, current_counter\n",
    "\n",
    "    return total_tokens, current_counter\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"STARTING....\")\n",
    "    total_tokens_for_all_languages = 0\n",
    "    \n",
    "    for lang in lang_dic:\n",
    "        current_counter = 0\n",
    "        #total_tokens for single language for all resources\n",
    "        total_tokens = 0\n",
    "        global resource\n",
    "        resource = 'Mandarin MACICDATA'\n",
    "        global copyright_short\n",
    "        copyright_short = 'https://www.openslr.org/123/'\n",
    "        global copyright_long\n",
    "        copyright_long = '''  @article{yang2022open,\n",
    "  title={Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational (RAMC) Speech Dataset},\n",
    "  author={Yang, Zehui and Chen, Yifan and Luo, Lei and Yang, Runyan and Ye, Lingxuan and Cheng, Gaofeng and Xu, Ji and Jin, Yaohui and Zhang, Qingqing and Zhang, Pengyuan and others},\n",
    "  journal={arXiv preprint arXiv:2203.16844},\n",
    "  year={2022}\n",
    "}'''\n",
    "\n",
    "        global duplicate_text_set\n",
    "        duplicate_text_set = set()\n",
    "\n",
    "        print(\"Language: \",lang_dic[lang][2])\n",
    "\n",
    "        total_tokens, current_counter = parse_data(lang, total_tokens,current_counter)\n",
    "            \n",
    "        total_tokens_for_all_languages += total_tokens\n",
    "        report.write(lang_dic[lang][0] + ';' + str(total_tokens) + ';' + str(current_counter+1) + '\\n')\n",
    "    # print(\"Total tokens collected for all languages: \", total_tokens_for_all_languages)\n",
    "    report.close()\n",
    "    print(\"END...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9172fffb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
