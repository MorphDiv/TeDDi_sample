---
title: "Compare word token counts from the 100LC corpus database and progress.py"
author: "Steven Moran"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  github_document:
  pandoc_args: --webtex
---

```{r warning=FALSE, message=FALSE}
library(dplyr)
library(testthat)
library(knitr)
```

Normnally, we would load and query the SQLite database with an R library like `RSQLite` or dump it first as an `Rdata` object and query that directly. For example:

```{r, eval = FALSE}
# Load the R serialized version of the 100 LC database
load('../../Database/100LC.Rdata')

# No NAs allows in the word.word_text field
expect_false(any(is.na(df$word_text)))

# Get word token and type counts per corpus and per writing system
db.token.counts <- df %>% select(name, writing_system, word_text) %>% group_by(name, writing_system) %>% summarize(db_tokens = n())
db.token.counts
```

However, when the current 100 LC corpus files are loaded into the database to the word level, there are 125,146,947 rows in the word table. Converting this on my laptop at the moment runs into memory issues.

As such, we can create a database view in SQLite that denormalizes the database tables into a 125,146,947 row table with pertinent metadata:

```
CREATE VIEW IF NOT EXISTS v_words
AS
SELECT 
word.id as word_id,
line.id as line_id,
line.file_id,
corpus.id as corpus_id,
language.id as language_id,
word.text as word_text,
line.text as line_text,
file.writing_system,
file.genre_broad as file_genre_broad,
file.genre_narrow as file_genre_narrow,
corpus.genre_broad,
corpus.genre_narrow,
language.name
FROM word
LEFT JOIN line ON word.line_id = line.id
LEFT JOIN file ON line.file_id = file.id
LEFT JOIN corpus ON file.corpus_id = corpus.id
LEFT JOIN language on corpus.language_id = language.id;
```

and then do the word token counts per folder:

`select name, count(*) from v_words group by name`

or do the word token counts per corpus and writing system:

`select name, writing_system, count(*) from v_words group by name, writing_system`

The result is saved to a CSV file (one row is one corpus and its counts) called `results_db_word_token_counts.csv` and `results_db_word_token_counts-ws.csv`, respectively. We can use this to compare with the output from the `progress.py` tokenization script (simple tokenization counts).

```{r}
# Read and rename
df <- read.csv('results_db_word_token_counts.csv')
df <- df %>% rename(db_tokens = "count...")

df.ws <- read.csv('results_db_word_token_counts-ws.csv')
df.ws <- df.ws %>% rename(db_tokens = "count...")
```


```{r}
# Load simple tokenization counts from progres.py
simple <- read.csv('progress_simple.csv')
simple <- simple %>% select(language, number_tokens) %>% rename(name=language, simple_number_tokens=number_tokens)
```

```{r}
# Combine language folder level
combined <- left_join(df, simple)
combined$delta <- abs(combined$db_tokens-combined$simple_number_tokens)
combined$diff <- round((combined$delta/combined$simple_number_tokens)*100, digits=2)
```

```{r}
# Combine writing systems level
combined.ws <- left_join(df.ws, simple)
combined.ws$delta <- abs(combined.ws$db_tokens-combined.ws$simple_number_tokens)
combined.ws$diff <- round((combined.ws$delta/combined.ws$simple_number_tokens)*100, digits=2)
```

Here is when word tokens are counted at the folder level:

```{r}
kable(combined)
```

Here is when word tokens are counted at the corpus and writing system levels:

```{r}
kable(combined.ws)
```

